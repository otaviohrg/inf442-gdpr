{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertTokenizer\n",
    "from gdpr.data.default_parse import read_examples_from_file, convert_examples_to_features\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "dataset = \"eng.train\"\n",
    "examples = read_examples_from_file(\".\", mode=f\"data/{dataset}\")\n",
    "pad_token_label_id = torch.nn.CrossEntropyLoss().ignore_index\n",
    "\n",
    "features = convert_examples_to_features(\n",
    "    examples,\n",
    "    label_list=labels,\n",
    "    max_seq_length=128,\n",
    "    tokenizer=tokenizer,\n",
    "    cls_token_at_end=False,\n",
    "    cls_token=tokenizer.cls_token,\n",
    "    cls_token_segment_id=0,\n",
    "    sep_token=tokenizer.sep_token,\n",
    "    sep_token_extra=False,\n",
    "    pad_on_left=False,\n",
    "    pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "    pad_token_segment_id=0,\n",
    "    pad_token_label_id=pad_token_label_id)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "tensordataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "\n",
    "dataset_test = \"eng.testa\"\n",
    "test_examples = read_examples_from_file(\".\", mode=f\"data/{dataset_test}\")\n",
    "\n",
    "test_features = convert_examples_to_features(\n",
    "    test_examples,\n",
    "    label_list=labels,\n",
    "    max_seq_length=128,\n",
    "    tokenizer=tokenizer,\n",
    "    cls_token_at_end=False,\n",
    "    cls_token=tokenizer.cls_token,\n",
    "    cls_token_segment_id=0,\n",
    "    sep_token=tokenizer.sep_token,\n",
    "    sep_token_extra=False,\n",
    "    pad_on_left=False,\n",
    "    pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "    pad_token_segment_id=0,\n",
    "    pad_token_label_id=pad_token_label_id)\n",
    "\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_ids for f in test_features], dtype=torch.long)\n",
    "\n",
    "test_tensordataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "eval_sampler = SequentialSampler(tensordataset)\n",
    "eval_dataloader = DataLoader(tensordataset, sampler=eval_sampler, batch_size=1)\n",
    "\n",
    "eval_sampler = SequentialSampler(test_tensordataset)\n",
    "test_eval_dataloader = DataLoader(test_tensordataset, sampler=eval_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 293513\n"
     ]
    }
   ],
   "source": [
    "from gdpr.models.lstm_model.lstm import LSTM\n",
    "\n",
    "# model = LSTM(vocab_size=len(tokenizer.vocab), patch_size=128)\n",
    "model = LSTM()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor = torch.rand(3, 3, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7d43ba3b7e4ed58436f57fb9fbb281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1111111111111111 batch:  0\n",
      "0.4411643314869121 batch:  5\n",
      "0.6273372466041087 batch:  10\n",
      "0.7113309870769549 batch:  15\n",
      "0.7485212677520892 batch:  20\n",
      "0.7604127821953687 batch:  25\n",
      "0.7646123683655054 batch:  30\n",
      "0.7800744868559306 batch:  35\n",
      "0.776697226554104 batch:  40\n",
      "0.78346503225051 batch:  45\n",
      "0.7956369945415431 batch:  50\n",
      "0.8051141230240427 batch:  55\n",
      "0.7955346468832674 batch:  60\n",
      "0.7874362920918353 batch:  65\n",
      "0.7948044047658956 batch:  70\n",
      "0.7993233296820909 batch:  75\n",
      "0.7970304998979795 batch:  80\n",
      "0.8028717499039109 batch:  85\n",
      "0.8044582489062251 batch:  90\n",
      "0.7894112336275443 batch:  95\n",
      "0.7939235403321128 batch:  100\n",
      "0.7979184386473614 batch:  105\n",
      "0.8002520049120573 batch:  110\n",
      "0.8070761180008232 batch:  115\n",
      "0.8088787693277332 batch:  120\n",
      "0.80807480728422 batch:  125\n",
      "0.8003637060918435 batch:  130\n",
      "0.7991549599598806 batch:  135\n",
      "0.7976559261628 batch:  140\n",
      "0.7968953723510437 batch:  145\n",
      "0.8019678921435744 batch:  150\n",
      "0.7992317836514581 batch:  155\n",
      "0.7974824395288321 batch:  160\n",
      "0.7950583891892277 batch:  165\n",
      "0.7883427817680402 batch:  170\n",
      "0.7918089756124039 batch:  175\n",
      "0.7939790079341127 batch:  180\n",
      "0.7968104560610516 batch:  185\n",
      "0.7936287377663724 batch:  190\n",
      "0.7942958866953166 batch:  195\n",
      "0.7906412863392378 batch:  200\n",
      "0.7870933858726262 batch:  205\n",
      "0.7838221591706118 batch:  210\n",
      "0.7814188684490699 batch:  215\n",
      "0.775332314013416 batch:  220\n",
      "0.7721396639472902 batch:  225\n",
      "0.7766470905960419 batch:  230\n",
      "0.7786327533031251 batch:  235\n",
      "0.7801673586827436 batch:  240\n",
      "0.7748084273377271 batch:  245\n",
      "0.7792943152393661 batch:  250\n",
      "0.7836049731448472 batch:  255\n",
      "0.7877504717436049 batch:  260\n",
      "0.7888553083231946 batch:  265\n",
      "0.7869391587231357 batch:  270\n",
      "0.7895384757016785 batch:  275\n",
      "0.7887011119432613 batch:  280\n",
      "0.7891926691089068 batch:  285\n",
      "0.7921043247323443 batch:  290\n",
      "0.7947436376313189 batch:  295\n",
      "0.794742926951411 batch:  300\n",
      "0.7948916105720092 batch:  305\n",
      "0.7973824715811475 batch:  310\n",
      "0.7991805508341184 batch:  315\n",
      "0.7996080708879738 batch:  320\n",
      "0.7986181454919149 batch:  325\n",
      "0.7922561037230363 batch:  330\n",
      "0.7866871435191514 batch:  335\n",
      "0.7810396231086982 batch:  340\n",
      "0.7758865135904285 batch:  345\n",
      "0.7740737306137722 batch:  350\n",
      "0.7717423693422417 batch:  355\n",
      "0.772515713794951 batch:  360\n",
      "0.7745864432675593 batch:  365\n",
      "0.7725182071200628 batch:  370\n",
      "0.772631131240941 batch:  375\n",
      "0.7728965644642507 batch:  380\n",
      "0.7735067125929522 batch:  385\n",
      "0.7735532106050994 batch:  390\n",
      "0.774055484881971 batch:  395\n",
      "0.7730667868754865 batch:  400\n",
      "0.7726349298942613 batch:  405\n",
      "0.7748289182073576 batch:  410\n",
      "0.775125095866775 batch:  415\n",
      "0.774166893355821 batch:  420\n",
      "0.7733621439138085 batch:  425\n",
      "0.7715719522100244 batch:  430\n",
      "0.7699598388933461 batch:  435\n",
      "0.7694185960739456 batch:  440\n",
      "0.7667719003033109 batch:  445\n",
      "0.7633603229057977 batch:  450\n",
      "0.7640278898064187 batch:  455\n",
      "0.7599284615662254 batch:  460\n",
      "0.7598745746921909 batch:  465\n",
      "0.7601821286110229 batch:  470\n",
      "0.7594550365348878 batch:  475\n",
      "0.7594904014654726 batch:  480\n",
      "0.7590841216150048 batch:  485\n",
      "0.7586618703523169 batch:  490\n",
      "0.7579905157324699 batch:  495\n",
      "0.7577535088313717 batch:  500\n",
      "0.7571753360587536 batch:  505\n",
      "0.7556550401819877 batch:  510\n",
      "0.7534518210744378 batch:  515\n",
      "0.7537957164027609 batch:  520\n",
      "0.754506512145809 batch:  525\n",
      "0.7546283518379001 batch:  530\n",
      "0.7548294910576071 batch:  535\n",
      "0.7542875408277815 batch:  540\n",
      "0.7544314278165382 batch:  545\n",
      "0.755833398266738 batch:  550\n",
      "0.7552884764769121 batch:  555\n",
      "0.7566631229390057 batch:  560\n",
      "0.7565485854483042 batch:  565\n",
      "0.7562063549931163 batch:  570\n",
      "0.755949558305879 batch:  575\n",
      "0.7553138956827417 batch:  580\n",
      "0.7552113030855342 batch:  585\n",
      "0.7551672142269426 batch:  590\n",
      "0.7554834049033703 batch:  595\n",
      "0.754193467262185 batch:  600\n",
      "0.7542963814486907 batch:  605\n",
      "0.7541487527759146 batch:  610\n",
      "0.7529516579211318 batch:  615\n",
      "0.7532346175038541 batch:  620\n",
      "0.7508924879710757 batch:  625\n",
      "0.7487459547858848 batch:  630\n",
      "0.7486796053481523 batch:  635\n",
      "0.7491319269335616 batch:  640\n",
      "0.7486441497213135 batch:  645\n",
      "0.7490307104682647 batch:  650\n",
      "0.749153912409778 batch:  655\n",
      "0.7497978746024856 batch:  660\n",
      "0.7490956663566992 batch:  665\n",
      "0.7483962805808527 batch:  670\n",
      "0.7483341779138345 batch:  675\n",
      "0.7467103794502441 batch:  680\n",
      "0.7457103802524325 batch:  685\n",
      "0.7437178762010135 batch:  690\n",
      "0.7426109018003385 batch:  695\n",
      "0.7406210691458167 batch:  700\n",
      "0.7397324745707864 batch:  705\n",
      "0.7378913325409082 batch:  710\n",
      "0.7368559673669287 batch:  715\n",
      "0.7357711103145894 batch:  720\n",
      "0.7348621922256199 batch:  725\n",
      "0.7341256028613737 batch:  730\n",
      "0.7323402976199833 batch:  735\n",
      "0.7315495500999533 batch:  740\n",
      "0.7297480823564587 batch:  745\n",
      "0.7291159282448877 batch:  750\n",
      "0.7295584931773405 batch:  755\n",
      "0.727957753576477 batch:  760\n",
      "0.7271225201980405 batch:  765\n",
      "0.7274654351124499 batch:  770\n",
      "0.7270307351439419 batch:  775\n",
      "0.7263638107374961 batch:  780\n",
      "0.7252419035445097 batch:  785\n",
      "0.7218164384989271 batch:  790\n",
      "0.7224511162901217 batch:  795\n",
      "0.7217699815650479 batch:  800\n",
      "0.7210736653613181 batch:  805\n",
      "0.7204475873351928 batch:  810\n",
      "0.7183381687620022 batch:  815\n",
      "0.7168866573809912 batch:  820\n",
      "0.7157553822152467 batch:  825\n",
      "0.7136549687642926 batch:  830\n",
      "0.7122901336966505 batch:  835\n",
      "0.7136226817152477 batch:  840\n",
      "0.7143917797387005 batch:  845\n",
      "0.7150868181742417 batch:  850\n",
      "0.715830371263697 batch:  855\n",
      "0.7164858290751643 batch:  860\n",
      "0.7173436330139009 batch:  865\n",
      "0.7180891273950426 batch:  870\n",
      "0.7188134274939482 batch:  875\n",
      "0.7189273631431817 batch:  880\n",
      "0.7191295896760187 batch:  885\n",
      "0.7199325786353128 batch:  890\n",
      "0.719810718476049 batch:  895\n",
      "0.7197793477081088 batch:  900\n",
      "0.7195121199314827 batch:  905\n",
      "0.7199017364000269 batch:  910\n",
      "0.7206118797602887 batch:  915\n",
      "0.7203848222088642 batch:  920\n",
      "0.7188002555496537 batch:  925\n",
      "0.7173222184284942 batch:  930\n",
      "0.7153917795263337 batch:  935\n",
      "0.7139606893378867 batch:  940\n",
      "0.7144354630664808 batch:  945\n",
      "0.7140090585989036 batch:  950\n",
      "0.7137614519465382 batch:  955\n",
      "0.7135164218462264 batch:  960\n",
      "0.7132739282893962 batch:  965\n",
      "0.7130339320915446 batch:  970\n",
      "0.7127963948711301 batch:  975\n",
      "0.7125612790291093 batch:  980\n",
      "0.7119904814681096 batch:  985\n",
      "0.7117618042995855 batch:  990\n",
      "0.7120451841384985 batch:  995\n",
      "0.7130453615491672 batch:  1000\n",
      "0.7135018578912792 batch:  1005\n",
      "0.7140250964739172 batch:  1010\n",
      "0.71509508796972 batch:  1015\n",
      "0.7152075147258866 batch:  1020\n",
      "0.7160251275728816 batch:  1025\n",
      "0.7158608100635218 batch:  1030\n",
      "0.7165216062397488 batch:  1035\n",
      "0.7176046180349693 batch:  1040\n",
      "0.7186529371269885 batch:  1045\n",
      "0.7197657400072792 batch:  1050\n",
      "0.7201331110174459 batch:  1055\n",
      "0.7212561479987094 batch:  1060\n",
      "0.721719299274513 batch:  1065\n",
      "0.7229335797719156 batch:  1070\n",
      "0.7241945097650094 batch:  1075\n",
      "0.7241751087022666 batch:  1080\n",
      "0.7246131962823318 batch:  1085\n",
      "0.7255163361787388 batch:  1090\n",
      "0.7264548356999828 batch:  1095\n",
      "0.7266899317981467 batch:  1100\n",
      "0.7275919704036128 batch:  1105\n",
      "0.726510307066115 batch:  1110\n",
      "0.727101242931078 batch:  1115\n",
      "0.7273852638782511 batch:  1120\n",
      "0.7277268603470892 batch:  1125\n",
      "0.7274740397140231 batch:  1130\n",
      "0.727453304599271 batch:  1135\n",
      "0.7280597980793367 batch:  1140\n",
      "0.7291313403608516 batch:  1145\n",
      "0.7299951180872292 batch:  1150\n",
      "0.7308601276391257 batch:  1155\n",
      "0.7313971449858804 batch:  1160\n",
      "0.7319486152046373 batch:  1165\n",
      "0.7324831764426315 batch:  1170\n",
      "0.7329929458981517 batch:  1175\n",
      "0.7331350952696709 batch:  1180\n",
      "0.7335303878538654 batch:  1185\n",
      "0.7343516988480416 batch:  1190\n",
      "0.7345083042591746 batch:  1195\n",
      "0.7352234494826223 batch:  1200\n",
      "0.7352281224727965 batch:  1205\n",
      "0.7352537230052316 batch:  1210\n",
      "0.7360715122034931 batch:  1215\n",
      "0.7366404249299325 batch:  1220\n",
      "0.7376057304291306 batch:  1225\n",
      "0.738671507316096 batch:  1230\n",
      "0.7386140128466089 batch:  1235\n",
      "0.7390151929273098 batch:  1240\n",
      "0.7392863735599988 batch:  1245\n",
      "0.7400077314392256 batch:  1250\n",
      "0.739962400858875 batch:  1255\n",
      "0.7398730699874291 batch:  1260\n",
      "0.7399323301173276 batch:  1265\n",
      "0.7402357571902866 batch:  1270\n",
      "0.7398914985507515 batch:  1275\n",
      "0.7394732393012187 batch:  1280\n",
      "0.7396346015502656 batch:  1285\n",
      "0.7399506826954882 batch:  1290\n",
      "0.7406396457002505 batch:  1295\n",
      "0.7410215071695041 batch:  1300\n",
      "0.7410014041915159 batch:  1305\n",
      "0.7417558211236975 batch:  1310\n",
      "0.741875722411814 batch:  1315\n",
      "0.742586395839989 batch:  1320\n",
      "0.7429790758941325 batch:  1325\n",
      "0.7438762926707067 batch:  1330\n",
      "0.7440635299507792 batch:  1335\n",
      "0.74416212570951 batch:  1340\n",
      "0.7441755792881305 batch:  1345\n",
      "0.744519027842304 batch:  1350\n",
      "0.7452506729331162 batch:  1355\n",
      "0.7452619979162177 batch:  1360\n",
      "0.7459041949208132 batch:  1365\n",
      "0.7466982582640501 batch:  1370\n",
      "0.7466687800675449 batch:  1375\n",
      "0.7470507070239144 batch:  1380\n",
      "0.7473598245860148 batch:  1385\n",
      "0.7481188777977603 batch:  1390\n",
      "0.7489046268027827 batch:  1395\n",
      "0.7487046000934836 batch:  1400\n",
      "0.748773575019407 batch:  1405\n",
      "0.7494727247876657 batch:  1410\n",
      "0.7501702382842084 batch:  1415\n",
      "0.750682485565935 batch:  1420\n",
      "0.7513214996169987 batch:  1425\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(train_step(model, eval_dataloader, torch.nn.CrossEntropyLoss(), optimizer, device))\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_eval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/X/X1P3/inf442-gdpr/gdpr/train/bert_train_steps.py:85\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m---> 85\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     91\u001b[0m       dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m     92\u001b[0m       loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m     93\u001b[0m       device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     96\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/X/X1P3/inf442-gdpr/gdpr/train/bert_train_steps.py:29\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 29\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m y_pred_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(torch\u001b[38;5;241m.\u001b[39msoftmax(preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (y_pred_class \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(y_pred_class\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    389\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    393\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gdpr.train.bert_train_steps import train\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                             lr=3e-3,\n",
    "                             betas=(0.9, 0.999),\n",
    "                             weight_decay=0.3)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "# print(train_step(model, eval_dataloader, torch.nn.CrossEntropyLoss(), optimizer, device))\n",
    "results = train(model, eval_dataloader, test_eval_dataloader, optimizer, torch.nn.CrossEntropyLoss(), epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
